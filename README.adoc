= LLM Tools
:app-name: LLM Tools
:author: Artem Durynin
:email: artem.durynin@raftds.com
:toc-title: Table of Contents
:toc: auto
:icons: font

[toc]

== Description

LLM Tools is a modular and extensible framework for working with Large Language Models (LLMs).  
It provides a **single configurable pipeline** for both *fine-tuning* and *inference* of different models, powered by Hugging Face, vLLM, Unsloth, and PEFT.  

With centralized YAML configuration, you can switch between models, trainers, and runners without modifying code.

== Key Features

* Unified pipeline for *training* and *inference*.
* Configurable via YAML – change the model or runner without touching code.
* Supports Hugging Face, vLLM, Unsloth, PEFT adapters.
* Dataset utilities with dialog/message abstractions.
* Metric evaluation, including Rouge and SVG-based visualization.
* Extensible architecture: add new runners, trainers, or metrics easily.

== Installation

=== Prerequisites

- Python 3.12+
- `rye` for environment and dependency management.

=== Steps

. Clone this repository:
+
[,bash]
----
git clone https://github.com/your-repo/llm-tools.git
cd llm-tools
----

. Set up environment with Rye:
+
[,bash]
----
rye sync
----

. Install the package:
+
[,bash]
----
pip install .
----

== Usage

LLM Tools operates through YAML configuration files.  
You only need to update the configuration to switch models or pipelines.

=== Inference

Check the `examples/` directory for runnable scripts.

== Configuration

All pipeline behavior is defined via YAML files.  

* `model_configs/` – predefined model configs (LLaMA 3, Qwen, etc.)

=== Quantization

This library can work with quantized models, allowing you to significantly reduce memory requirements and improve inference speed, especially when working with large language models.

==== How to Use Quantized Models

Quantize the Model before using a quantized model with this library, you must first quantize your pretrained model using external tools or libraries. One of the most popular tools for this purpose is bitsandbytes (bnb), which supports 8-bit and 4-bit quantization for PyTorch models. Other options include AutoGPTQ or HuggingFace’s built-in quantization features.

==== Supported Quantization Level

This library currently supports models quantized to 4 bits.
This provides an optimal balance between memory efficiency and preserving model performance.

==== Configure the Model Path and Quantization Settings

Once you've quantized your model, save it to a directory. In your configuration file (for example, model_configs/qwen8b-train.yaml), specify the path to your quantized model and set the following parameter: `load_in_4bit: True`. Then you can run the script to train the model.

== Future Plans

* Extend example scripts for more use cases (chatbots, alignment, evaluation).
* Provide detailed YAML schema documentation.
* Add adapter/dataset storage integration with DVC.

== Dependencies

Core dependencies:

* Python 3.12+
* torch
* transformers
* vllm
* unsloth
* peft
* datasets
* evaluate
* pandas
* pydantic

(see `pyproject.toml` for the complete list)

== License

This project is licensed under the **GPLv3** license.
